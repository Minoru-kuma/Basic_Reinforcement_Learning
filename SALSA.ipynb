{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA+AKLK85U6/CTRPArajPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minoru-kuma/Basic_Reinforcement_Learning/blob/main/SALSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz7r35AQtZ_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12830380-4641-40cd-ccec-0afbdd60dead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[500, 500, 500, 500, 500, 118, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 175, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 129, 500, 500, 500, 500, 500, 500, 500, 500, 500, 164, 500, 490, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 341, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 262, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 433, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 121, 500, 500, 500, 84, 500, 500, 500, 500, 500, 500, 500, 207, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 117, 500, 500, 500, 500, 500, 500, 208, 500, 500, 500, 500, 500, 500, 102, 500, 500, 500, 45, 500, 500, 500, 500, 500, 500, 220, 500, 500, 500, 145, 500]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#迷路の盤面の実装\n",
        "MOVE = [[0, 1, 5, 0], [1, 2, 1, 0], [2, 3, 2, 1], [3, 4, 8, 2], [4, 4, 9, 3],\n",
        " [0, 5, 10, 5], [6, 7, 11, 6], [2, 7, 12, 6], [3, 8, 13, 8], [4, 9, 14, 9],\n",
        " [5, 10, 15, 10], [6, 11, 16, 11], [7, 12, 17, 12], [8, 13, 18, 13],\n",
        "[9, 14, 19, 14], [10, 15, 20, 15], [11, 16, 21, 16], [12, 17, 22, 17],\n",
        "[13, 18, 18, 18], [14, 19, 19, 19],[15, 21, 20, 20], [16, 21, 21, 20],\n",
        "[17, 23, 22, 22], [23, 24, 23, 22], [0, 0, 0, 0]]\n",
        "\n",
        "StateN = 25\n",
        "ActionN = 4\n",
        "\n",
        "\n",
        "# パラメータ設定\n",
        "#エピソード数\n",
        "episodes = 200\n",
        "#学習系数\n",
        "alpha = 0.2\n",
        "#ε-greedy法のε\n",
        "epsiron = 0.5\n",
        "#割引率γ\n",
        "ganma = 0.8\n",
        "#アクション数\n",
        "actionscounts = 0\n",
        "#状態数\n",
        "states = 0\n",
        "#報酬\n",
        "cookies = 0\n",
        "#エピソードごとのカウント\n",
        "epcount = list()\n",
        "\n",
        "#Q(s, a)をランダム値で初期化する．S;State A;Action\n",
        "Q = 0.1 * np.random.rand(StateN,ActionN)\n",
        "\n",
        "\n",
        "#アクション選択関数\n",
        "'''\n",
        "動作：Q値からアクション選択\n",
        "'''\n",
        "def select_action(stateNo):\n",
        "  rand = np.random.rand()\n",
        "  if epsiron > rand:\n",
        "    return int((rand * 100)) % 4\n",
        "\n",
        "  else:\n",
        "    tempcount = 0\n",
        "    tempno = 0\n",
        "    for i in range(ActionN):\n",
        "      if Q[stateNo][i] > tempcount:\n",
        "        tempcount = Q[stateNo][i]\n",
        "        tempno = i\n",
        "    return tempno\n",
        "  #print(int((rand * 100)) % 4)\n",
        "\n",
        "select_action(0)\n",
        "\n",
        "def update_state(action,oldstate):\n",
        "  temp = 0\n",
        "  if action == 0:\n",
        "    temp = MOVE[oldstate][0]\n",
        "  elif action == 1:\n",
        "    temp = MOVE[oldstate][1]\n",
        "  elif action == 2:\n",
        "    temp = MOVE[oldstate][2]\n",
        "  else:\n",
        "    temp = MOVE[oldstate][3]\n",
        "  #print(temp)\n",
        "  return temp\n",
        "\n",
        "\n",
        "#Q値更新関数\n",
        "def update_q(state,oldsta,action,oldact,rewards):\n",
        "  Q[oldstate,oldaction] = Q[oldstate,oldaction] + alpha*(rewards + (ganma * Q[state,action]) -Q[oldstate,oldaction])\n",
        "\n",
        "#update_q(\"update_Q\")\n",
        "\n",
        "#メインパート\n",
        "for i in range(episodes):\n",
        "  state = 0\n",
        "  action = select_action(state)\n",
        "  rpcount = 0\n",
        "  cookies = 0\n",
        "  #ゴール判定用フラッグ\n",
        "  flag = False\n",
        "  #500ステップの制限\n",
        "  for i in range(500):\n",
        "    #状態更新\n",
        "    oldstate = state\n",
        "    state = update_state(action,oldstate)\n",
        "    #報酬与えた後処理続行\n",
        "    if oldstate == state:\n",
        "      cookies -= 1\n",
        "      oldaction = action\n",
        "    action = select_action(state)\n",
        "\n",
        "\n",
        "    #ゴール到着時終了\n",
        "    if state == 24:\n",
        "      cookies += 10\n",
        "      flag = True\n",
        "      break\n",
        "    rpcount += 1\n",
        "  #未ゴール時の減点\n",
        "  if flag == False:\n",
        "    cookies -= 10\n",
        "\n",
        "  epcount.append(rpcount)\n",
        "print(epcount)"
      ]
    }
  ]
}